{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c4114ed",
   "metadata": {},
   "source": [
    "<font color=\"red\"> Remaining to do in Lasso/Linear:\n",
    "\n",
    "- <s>Outliers to be identified and removed </s>\n",
    "\n",
    "- <s>Analysis to be done after removing outliers as they have significant influence </s>\n",
    "- Maybe Better plots required to visualize \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7fd31d",
   "metadata": {},
   "source": [
    "# Data Science in Engineering\n",
    "## Semesteraufgabe SoSe 2023\n",
    "### Aufgabe 1: Glanzoptimierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd1464a",
   "metadata": {},
   "source": [
    "| Name              | Matrik No. |\n",
    "|-------------------|:------------:|\n",
    "| Renuka Gajralwar  |            |\n",
    "| Saikiran Joshi    |            |\n",
    "| Hithesh Karanam   |     478001       |\n",
    "| Taher Muhammedali |   478012   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfbbd44",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "leading-profit",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'MachAr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ffef894c0435>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mzscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\hit\\tu berlin\\sem-02\\dse\\.venv\\lib\\site-packages\\statsmodels\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0madd_constant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mregression\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOLS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGLS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWLS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGLSAR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mregression\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecursive_ls\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRecursiveLS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\hit\\tu berlin\\sem-02\\dse\\.venv\\lib\\site-packages\\statsmodels\\regression\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0myule_walker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_testing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPytestTester\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0m__all__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'yule_walker'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\hit\\tu berlin\\sem-02\\dse\\.venv\\lib\\site-packages\\statsmodels\\regression\\linear_model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m from statsmodels.tools.decorators import (cache_readonly,\n\u001b[0;32m     45\u001b[0m                                           cache_writable)\n\u001b[1;32m---> 46\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapper\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memplike\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melregress\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_ELRegOpts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\hit\\tu berlin\\sem-02\\dse\\.venv\\lib\\site-packages\\statsmodels\\base\\model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m                                           cached_value, cached_data)\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapper\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumdiff\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapprox_fprime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msm_exceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mValueWarning\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mHessianInversionWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\hit\\tu berlin\\sem-02\\dse\\.venv\\lib\\site-packages\\statsmodels\\tools\\numdiff.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;31m# NOTE: we only do double precision internally so far\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m \u001b[0mEPS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMachAr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m _hessian_docs = \"\"\"\n",
      "\u001b[1;32mf:\\hit\\tu berlin\\sem-02\\dse\\.venv\\lib\\site-packages\\numpy\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    318\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mTester\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m         raise AttributeError(\"module {!r} has no attribute \"\n\u001b[0m\u001b[0;32m    321\u001b[0m                              \"{!r}\".format(__name__, attr))\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'MachAr'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "import warnings\n",
    "from warnings import simplefilter\n",
    "\n",
    "from sklearn.linear_model import Lasso, LassoCV, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f334cd52",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attempted-favor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in the Gloss Dataset is 58.\n",
      "Number of features in the Gloss Dataset is 14.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V13</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V21</th>\n",
       "      <th>V28</th>\n",
       "      <th>V29</th>\n",
       "      <th>V30</th>\n",
       "      <th>V31</th>\n",
       "      <th>V32</th>\n",
       "      <th>V33</th>\n",
       "      <th>V34</th>\n",
       "      <th>V39</th>\n",
       "      <th>gloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10</td>\n",
       "      <td>HMW 1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>LMW 1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>LMW 6</td>\n",
       "      <td>-0.004681</td>\n",
       "      <td>0.103880</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>-0.005881</td>\n",
       "      <td>0.055534</td>\n",
       "      <td>0.058750</td>\n",
       "      <td>-0.004681</td>\n",
       "      <td>8</td>\n",
       "      <td>6200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.12</td>\n",
       "      <td>HMW 1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>LMW 1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>LMW 6</td>\n",
       "      <td>-0.010345</td>\n",
       "      <td>0.112653</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.005503</td>\n",
       "      <td>0.040954</td>\n",
       "      <td>0.056777</td>\n",
       "      <td>-0.010345</td>\n",
       "      <td>8</td>\n",
       "      <td>6400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.12</td>\n",
       "      <td>HMW 1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>LMW 1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>LMW 6</td>\n",
       "      <td>-0.005359</td>\n",
       "      <td>0.125634</td>\n",
       "      <td>0.003815</td>\n",
       "      <td>-0.012599</td>\n",
       "      <td>0.074109</td>\n",
       "      <td>0.015341</td>\n",
       "      <td>-0.005359</td>\n",
       "      <td>8</td>\n",
       "      <td>5800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.14</td>\n",
       "      <td>HMW 1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>LMW 1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>LMW 6</td>\n",
       "      <td>-0.004240</td>\n",
       "      <td>0.147946</td>\n",
       "      <td>-0.016650</td>\n",
       "      <td>0.010005</td>\n",
       "      <td>0.039433</td>\n",
       "      <td>0.018663</td>\n",
       "      <td>-0.004240</td>\n",
       "      <td>6</td>\n",
       "      <td>6200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.28</td>\n",
       "      <td>HMW 1</td>\n",
       "      <td>0.08</td>\n",
       "      <td>LMW 1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>LMW 6</td>\n",
       "      <td>-0.026882</td>\n",
       "      <td>0.284841</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>-0.019216</td>\n",
       "      <td>0.068883</td>\n",
       "      <td>0.038201</td>\n",
       "      <td>-0.026882</td>\n",
       "      <td>8</td>\n",
       "      <td>8600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    V13    V15   V16    V18   V19    V21       V28       V29       V30  \\\n",
       "0  0.10  HMW 1  0.05  LMW 1  0.05  LMW 6 -0.004681  0.103880  0.000972   \n",
       "1  0.12  HMW 1  0.04  LMW 1  0.04  LMW 6 -0.010345  0.112653  0.001479   \n",
       "2  0.12  HMW 1  0.06  LMW 1  0.02  LMW 6 -0.005359  0.125634  0.003815   \n",
       "3  0.14  HMW 1  0.04  LMW 1  0.02  LMW 6 -0.004240  0.147946 -0.016650   \n",
       "4  0.28  HMW 1  0.08  LMW 1  0.04  LMW 6 -0.026882  0.284841 -0.009568   \n",
       "\n",
       "        V31       V32       V33       V34  V39  gloss  \n",
       "0 -0.005881  0.055534  0.058750 -0.004681    8   6200  \n",
       "1  0.005503  0.040954  0.056777 -0.010345    8   6400  \n",
       "2 -0.012599  0.074109  0.015341 -0.005359    8   5800  \n",
       "3  0.010005  0.039433  0.018663 -0.004240    6   6200  \n",
       "4 -0.019216  0.068883  0.038201 -0.026882    8   8600  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "\n",
    "df = pd.read_excel('Aufgabe_1_Gloss_Optimization.xlsx')\n",
    "print(f\"Number of observations in the Gloss Dataset is {df.shape[0]}.\")\n",
    "print(f\"Number of features in the Gloss Dataset is {df.shape[1]-1}.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baf02566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of outliers: [37 38 40]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V13</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V21</th>\n",
       "      <th>V28</th>\n",
       "      <th>V29</th>\n",
       "      <th>V30</th>\n",
       "      <th>V31</th>\n",
       "      <th>V32</th>\n",
       "      <th>V33</th>\n",
       "      <th>V34</th>\n",
       "      <th>V39</th>\n",
       "      <th>gloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.2</td>\n",
       "      <td>LMW 3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>HMW 4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007180</td>\n",
       "      <td>0.215330</td>\n",
       "      <td>-0.002961</td>\n",
       "      <td>0.025634</td>\n",
       "      <td>0.201124</td>\n",
       "      <td>0.014473</td>\n",
       "      <td>0.007180</td>\n",
       "      <td>8</td>\n",
       "      <td>11800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.2</td>\n",
       "      <td>LMW 3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>HMW 1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.031322</td>\n",
       "      <td>0.195481</td>\n",
       "      <td>0.008419</td>\n",
       "      <td>0.006735</td>\n",
       "      <td>0.183201</td>\n",
       "      <td>0.006038</td>\n",
       "      <td>0.031322</td>\n",
       "      <td>8</td>\n",
       "      <td>13600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.2</td>\n",
       "      <td>LMW 3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>HMW 6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.199251</td>\n",
       "      <td>-0.002632</td>\n",
       "      <td>-0.001479</td>\n",
       "      <td>-0.001118</td>\n",
       "      <td>0.189497</td>\n",
       "      <td>-0.000656</td>\n",
       "      <td>-0.000749</td>\n",
       "      <td>8</td>\n",
       "      <td>13400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    V13    V15  V16    V18  V19  V21       V28       V29       V30       V31  \\\n",
       "37  0.2  LMW 3  0.2  HMW 4  0.0  NaN  0.007180  0.215330 -0.002961  0.025634   \n",
       "38  0.2  LMW 3  0.2  HMW 1  0.0  NaN  0.031322  0.195481  0.008419  0.006735   \n",
       "40  0.2  LMW 3  0.2  HMW 6  0.0  NaN  0.199251 -0.002632 -0.001479 -0.001118   \n",
       "\n",
       "         V32       V33       V34  V39  gloss  \n",
       "37  0.201124  0.014473  0.007180    8  11800  \n",
       "38  0.183201  0.006038  0.031322    8  13600  \n",
       "40  0.189497 -0.000656 -0.000749    8  13400  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an array using np.arange\n",
    "original_array = np.arange(df.shape[1])\n",
    "\n",
    "# Values to be removed\n",
    "values_to_remove = np.array([1, 3, 5]) # String columns\n",
    "\n",
    "# Remove specified values using set differences\n",
    "filtered_array = np.setdiff1d(original_array, values_to_remove)\n",
    "\n",
    "# Define a threshold for outlier detection (e.g., 2 or 3)\n",
    "threshold = 2.5\n",
    "outlierlist = []\n",
    "\n",
    "for i in filtered_array:\n",
    "    # Create a sample dataset (replace this with your actual data)\n",
    "    data = df.iloc[:,i]\n",
    "\n",
    "    # Calculate Z-Scores\n",
    "    z_scores = zscore(data)\n",
    "    \n",
    "    outliers = np.where(np.abs(z_scores) > threshold)[0]\n",
    "    outlierlist = outlierlist + [item for item in outliers if item not in outlierlist]\n",
    "\n",
    "values = df.iloc[outliers]\n",
    "print(\"Indices of outliers:\", outliers)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f546f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in the Gloss Dataset without outliers is 55.\n",
      "Number of features in the Gloss Dataset without outliers is 14.\n"
     ]
    }
   ],
   "source": [
    "df2 = df.copy()\n",
    "df2 = df2.drop(outliers).reset_index(drop=True)\n",
    "print(f\"Number of observations in the Gloss Dataset without outliers is {df2.shape[0]}.\")\n",
    "print(f\"Number of features in the Gloss Dataset without outliers is {df2.shape[1]-1}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc7a9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_transformer(df):\n",
    "    \n",
    "    # Define features and target seperately \n",
    "    X = df.drop('gloss',axis=1) # Here X is our list of features\n",
    "    y = df['gloss'] # Here y is our target, gloss\n",
    "\n",
    "    # Finding column index where features are non-numeric\n",
    "    string_col_indices = [i for i,col in enumerate (X.columns) if X[col].dtype=='object']\n",
    "\n",
    "    # Using OneHotEncoder & ColumnTransformer to transform string indices\n",
    "    ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(drop='first'),string_col_indices )], remainder='passthrough')\n",
    "    X = np.array(ct.fit_transform(X)).astype('float64')\n",
    "    print(f\"After encoder & column transformation, the original dataframe has changed from {df.shape} to an array of {X.shape}\")\n",
    "    \n",
    "    # Creating a list with all feature names after column transformation\n",
    "    name_list = ct.get_feature_names_out().tolist()\n",
    "    name_list.insert(0, \"const\") # Manually adding a string of \"const\" to list of names\n",
    "    \n",
    "    X = sm.add_constant(X)\n",
    "    X_df = pd.DataFrame(X, columns=name_list)\n",
    "    \n",
    "    return X_df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee91c34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After encoder & column transformation, the original dataframe has changed from (58, 15) to an array of (58, 35)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ColumnTransformer' object has no attribute 'get_feature_names_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-0b8badc3a546>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_all_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_transformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mols_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_all_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_pred_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mols_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_all_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-951292d9db05>\u001b[0m in \u001b[0;36mencoder_transformer\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Creating a list with all feature names after column transformation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mname_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mname_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"const\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Manually adding a string of \"const\" to list of names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ColumnTransformer' object has no attribute 'get_feature_names_out'"
     ]
    }
   ],
   "source": [
    "X_all_df, y_all = encoder_transformer(df)\n",
    "\n",
    "ols_reg = sm.OLS(y_all, X_all_df).fit()\n",
    "\n",
    "y_pred_all = ols_reg.predict(X_all_df)\n",
    "rmse_all = mean_squared_error(y_all, y_pred_all, squared=False)\n",
    "r2_all = r2_score(y_all, y_pred_all)\n",
    "print(\"\\nRMSE:\", rmse_all)\n",
    "print(\"R2:\", r2_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6947cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ohne_df, y_ohne = encoder_transformer(df2)\n",
    "\n",
    "ols_reg2 = sm.OLS(y_ohne, X_ohne_df).fit()\n",
    "\n",
    "y_pred_ohne = ols_reg2.predict(X_ohne_df)\n",
    "rmse_ohne = mean_squared_error(y_ohne, y_pred_ohne, squared=False)\n",
    "r2_ohne = r2_score(y_ohne, y_pred_ohne)\n",
    "print(\"\\nRMSE:\", rmse_ohne)\n",
    "print(\"R2:\", r2_ohne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12a603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ols_reg2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820066a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, y_all, name_list_all = encoder_transformer(df)\n",
    "X_all = sm.add_constant(X_all) # Adding constant\n",
    "X_all_df = pd.DataFrame(X_all, columns=name_list_all) # Creating a dataframe of the transformed array with the feature name list combined\n",
    "\n",
    "ols_reg = sm.OLS(y_all, X_all_df).fit()\n",
    "\n",
    "y_pred_all = ols_reg.predict(X_all_df)\n",
    "rmse_all = mean_squared_error(y_all, y_pred_all, squared=False)\n",
    "r2_all = r2_score(y_all, y_pred_all)\n",
    "print(\"\\nRMSE:\", rmse_all)\n",
    "print(\"R2:\", r2_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-practice",
   "metadata": {},
   "source": [
    "Is the pair plot required?\n",
    "\n",
    "Don't think so...\n",
    "\n",
    "We can remove it no problem -Hithesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # checking pair plot of the dataset\n",
    "# sns.pairplot(df, kind='reg')\n",
    "# plt.suptitle('Pair plot for Gloss Optimization dataset', fontsize=16, y=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018563e4",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we will perform two correlation analysis, Pearson & Spearman\n",
    "\n",
    "# removing FutureWarning generated from correlation formulas\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#Checking the relationship between the variables of the given dataset\n",
    "\n",
    "# Finding the Pearson correlation of the dataset\n",
    "df_pears = df.corr(method='pearson')\n",
    "\n",
    "# Finding the Spearman correlation of the dataset\n",
    "df_spear = df.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9a9d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting both Correlation analysis to visualize and analyse correlation within the gloss dataset\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Subplot for Pearson correlation\n",
    "#Pearson correlation gives the linear realtionship between the variables\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(df_pears, annot=True, cmap='coolwarm')\n",
    "plt.title('Pearson Correlation', fontsize=18, y=1)\n",
    "\n",
    "\n",
    "# Subplot for Spearman correlation\n",
    "#Spearman correlation gives the monotonous realtionship (non-linear) between the variables\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(df_spear, annot=True, cmap='coolwarm')\n",
    "plt.title('Spearman Correlation', fontsize=18, y=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-fellowship",
   "metadata": {},
   "source": [
    "<b>Pearson Correlation Observation</b>\n",
    "- Here we can observe that features <i>V32 and V39</i> are showing medium-strong correlation to our target <i>gloss</i>, <i>0.42 & 0.55</i> respectively.\n",
    "- Subsequently, we can also observe that both <i>V32 and V39</i> are strongly correlated as per this correlation method with a strong correlation value of 0.73\n",
    "\n",
    "<b>Spearman Correlation Observation</b>\n",
    "- Spearman also reinforces the observations above from Pearson, for features <i>V32 and V39</i>.\n",
    "- Additionally, we can observe a medium correlation of feature <i>V19</i> with our target <i>gloss</i>\n",
    "\n",
    "Note - Non-numeric features such as <i>V15, V18 and V21</i> are not considered in the above correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cafaa9",
   "metadata": {},
   "source": [
    "## Transforming Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target seperately \n",
    "X = df.drop('gloss',axis=1) # Here X is our list of features\n",
    "y = df['gloss'] # Here y is our target, gloss\n",
    "\n",
    "# Finding column index where features are non-numeric\n",
    "string_col_indices = [i for i,col in enumerate (X.columns) if X[col].dtype=='object']\n",
    "\n",
    "# Using OneHotEncoder & ColumnTransformer to transform string indices\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(drop='first'),string_col_indices )], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X)).astype('float64')\n",
    "print(f\"After encoder & column transformation, the original dataframe has changed from {df.shape} to an array of {X.shape}\")\n",
    "\n",
    "# Creating a list with all feature names after column transformation\n",
    "name_list = ct.get_feature_names_out().tolist()\n",
    "name_list.insert(0, \"const\") # Manually adding a string of \"const\" to list of names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c661133",
   "metadata": {},
   "source": [
    "## OLS Regression\n",
    "(of the full dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d7f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS Regression Analysis of the dataset\n",
    "X_sm = sm.add_constant(X) # Adding constant\n",
    "X_sm_df = pd.DataFrame(X_sm, columns=name_list) # Creating a dataframe of the transformed array with the feature name list combined\n",
    "ols_reg = sm.OLS(y, X_sm_df).fit()\n",
    "print(ols_reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d003e7f7",
   "metadata": {},
   "source": [
    "Not sure which one of the below two is correct, ohne constant or with constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e354ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_p = ols_reg.summary2().tables[1]['P>|t|'].max()\n",
    "\n",
    "while max_p > 0.05:\n",
    "    max_index = ols_reg.summary2().tables[1]['P>|t|'][ols_reg.summary2().tables[1]['P>|t|'] == max_p].index[0]\n",
    "    X_sm_df.drop(columns=max_index, inplace=True)\n",
    "    ols_reg = sm.OLS(y, X_sm_df).fit()\n",
    "    max_p = ols_reg.summary2().tables[1]['P>|t|'].max()\n",
    "\n",
    "print(ols_reg.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bedf1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_p = ols_reg.summary2().tables[1][1:]['P>|t|'].max()\n",
    "\n",
    "while max_p > 0.05:\n",
    "    max_index = ols_reg.summary2().tables[1][1:]['P>|t|'][ols_reg.summary2().tables[1][1:]['P>|t|'] == max_p].index[0]\n",
    "    X_sm_df.drop(columns=max_index, inplace=True)\n",
    "    ols_reg = sm.OLS(y, X_sm_df).fit()\n",
    "    max_p = ols_reg.summary2().tables[1][1:]['P>|t|'].max()\n",
    "\n",
    "print(ols_reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c736a777",
   "metadata": {},
   "source": [
    "### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we only have 58 observations, it would be better to train the model on the basis of a siginificant training split of dataset for accurate predictions\n",
    "# Here we are using sklearn train_test_split to split the data using a pre-assigned random state in order to get consistent results at every run\n",
    "\n",
    "# Split the data into 90% training and 10% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae33aa3",
   "metadata": {},
   "source": [
    "### Scalar transformation of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f2d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scaler object\n",
    "sc = StandardScaler()\n",
    "# fit scaler based on training data, scale training data\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "# scale test data\n",
    "X_test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891e604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65a27fd4",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f3c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regressor and perform the fit based on the scaled training data\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train_sc, y_train)\n",
    "\n",
    "# Predict based on the features for both testing and training data\n",
    "y_pred_train_linear = linear_reg.predict(X_train_sc) # Predicting y for training data. This is expected to have a good fit as the model is trained using this dataset.\n",
    "y_pred_test_linear = linear_reg.predict(X_test_sc)  # predicting y for testing data\n",
    "\n",
    "# Print the metrics R2 and RMSE for both training and testing data\n",
    "print(f'Train metrics: R2: {r2_score(y_train, y_pred_train_linear)}, RMSE: {mean_squared_error(y_train, y_pred_train_linear, squared=False)}')\n",
    "print(f'Test metrics: R2: {r2_score(y_test, y_pred_test_linear)}, RMSE: {mean_squared_error(y_test, y_pred_test_linear, squared=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af9310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals for OLS regression\n",
    "linear_train_residuals = y_train - y_pred_train_linear\n",
    "linear_test_residuals = y_test - y_pred_test_linear\n",
    "\n",
    "# Define threshold for identifying outliers\n",
    "outlier_threshold = 2.5 # The outlier threshold can be tweaked as per requirements\n",
    "\n",
    "# Create Residuals vs. Predicted Values plot for HuberRegressor with marked outliers\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Create Residuals vs. Predicted Values plot for OLS regression with marked outliers\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_pred_train_linear, linear_train_residuals, label='Train')\n",
    "plt.scatter(y_pred_test_linear, linear_test_residuals, label='Test')\n",
    "\n",
    "# Mark outliers for OLS regression using outlier threshold\n",
    "ols_train_outliers = np.abs(linear_train_residuals) > outlier_threshold * np.std(linear_train_residuals)\n",
    "ols_test_outliers = np.abs(linear_test_residuals) > outlier_threshold * np.std(linear_test_residuals)\n",
    "plt.scatter(y_pred_train_linear[ols_train_outliers], linear_train_residuals[ols_train_outliers],\n",
    "            color='r', label='Train Outliers', marker='s')\n",
    "plt.scatter(y_pred_test_linear[ols_test_outliers], linear_test_residuals[ols_test_outliers],\n",
    "            color='r', label='Test Outliers')\n",
    "\n",
    "plt.axhline(0, color='r', linestyle='--') \n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('OLS Regression Residuals vs. Predicted Values')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5749453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residuals Plot for Linear Regression\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.residplot(x=y_test, y=y_pred_test_linear, label='Testing Data', scatter_kws={\"marker\": \"*\", \"color\": \"red\"}) #Plotting testing data\n",
    "sns.residplot(x=y_train, y=y_pred_train_linear, lowess=True, line_kws={\"color\":\"g\"}, label='Training Data', scatter_kws={\"marker\": \"o\", \"color\": \"blue\"}) #Plotting training data using lowess\n",
    "plt.legend()\n",
    "plt.xlabel('Predicted Dissolution')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title(\"Linear Regression Residuals Plot\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e86e930",
   "metadata": {},
   "source": [
    "<b>Linear Regression Observation</b>\n",
    "\n",
    "- As expected, predicted y values based on training data shows a healthy fit with R2 ≈ 1\n",
    "- However, the prediction of y values based on test data shows a poor fit with R2 ≈ 0.5\n",
    "- The RMSE for testing data being significantly higher than that of training data also shows that errors are larger for testing data\n",
    "\n",
    "From this we can possibly conclude that Linear Regression has not turned out to be good model for our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5cbd14",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2061453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suppressing Convergence warnings\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "#Defining alpha range for Lasso regression\n",
    "alphas = np.logspace(-5, 100, 1000)  \n",
    "lassocv_reg = LassoCV(alphas=alphas, cv=5) #cross validation = 5\n",
    "\n",
    "# Fitting the lasso regressor model based on training data\n",
    "lassocv_reg.fit(X_train_sc, y_train)\n",
    "\n",
    "# Predicting y values based on training and testing data\n",
    "y_pred_test_lasso = lassocv_reg.predict(X_test_sc)\n",
    "y_pred_train_lasso = lassocv_reg.predict(X_train_sc)\n",
    "\n",
    "# Calculate RMSE with the best alpha\n",
    "print(f'Train metrics: R2: {r2_score(y_train, y_pred_train_lasso)}, RMSE: {mean_squared_error(y_train, y_pred_train_lasso, squared=False)}')\n",
    "print(f'Test metrics: R2: {r2_score(y_test, y_pred_test_lasso)}, RMSE: {mean_squared_error(y_test, y_pred_test_lasso, squared=False)} \\n')\n",
    "\n",
    "# Get the best alpha value\n",
    "best_alpha = lassocv_reg.alpha_\n",
    "print(f\"Best Alpha: {best_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888a888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residuals Plot for Lasso Regression\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.residplot(x=y_test, y=y_pred_test_lasso, label='Testing Data', scatter_kws={\"marker\": \"*\", \"color\": \"red\"}) #Plotting testing data\n",
    "sns.residplot(x=y_train, y=y_pred_train_lasso, lowess=True, line_kws={\"color\":\"g\"}, label='Training Data', scatter_kws={\"marker\": \"o\", \"color\": \"blue\"}) #Plotting training data using lowess\n",
    "plt.legend()\n",
    "plt.xlabel('Predicted Dissolution')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title(\"Lasso Regression Residuals Plot\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c0b627",
   "metadata": {},
   "source": [
    "<b>Lasso Regression Observation</b>\n",
    "\n",
    "- As expected, again the predicted y values based on training data shows a healthy fit with R2 ≈ 1 for Lasso regression model\n",
    "- The prediction of y values based on test data shows a poor fit with R2 ≈ 0.6. However, this is still a better fit in comparison to Linear Regression model.\n",
    "- The RMSE for testing is also in Lasso's case significantly higher than that of training data\n",
    "- Linear Regressor performed slightly better than Lasso on training data with better R2 and lower RMSE values, however Lasso had a slightly better prediction for training dataset\n",
    "\n",
    "From this we can possibly conclude that Lasso Regression is also not a good model for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-container",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e9ab42e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
